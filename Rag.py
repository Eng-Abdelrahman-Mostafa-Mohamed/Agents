import os
import pandas as pd
from ollama import *
from llama_index.core import ServiceContext, set_global_service_context
from llama_index.experimental.query_engine import PandasQueryEngine
from llama_index.core import Settings
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.groq import Groq
from adapter.Adapter import Adapter
from sentence_transformers import SentenceTransformer
from prompt import new_prompt, instruction_str
from nomic import embed 
from Notes import note_engine
from llama_index.core.tools import QueryEngineTool, ToolMetadata
from llama_index.core.agent import ReActAgent
# from llama_index import ConversationBufferMemory
#
# from code_runner_agent import code_runner_engine  #try to make code runner agent 

# Set your API keys
os.environ['GROQ-API-KEY'] = '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'
os.environ['OPENAI_API_KEY'] = 'sk-mFBq7t6V5-OGh223N1le4a4q8RCoLjUUFU4Fms-7B5T3BlbkFJTOnS6lFV5F03K66Okiy1uKKOgBVLFSw7BLlKh4gtsA'

# Initialize the Groq model
# for reducting using open AI  and making model more fast 
llm = Groq(model="llama3-70b-8192", api_key=os.getenv('GROQ-API-KEY'))

# def nomic_embed_model(text):  #i tried to use this embeding model replaced to openAI 
#     return embed(text)
# 
Settings.llm = llm
Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)
Settings.num_output = 512
Settings.context_window = 3900



# Load the population data
data = pd.read_csv('WorldPopulation2023.csv')
print(data)

# Initialize the Pandas Query Engine
population_pandas_query_engine = PandasQueryEngine(df=data,llm=llm , verbose=True, instruction_str=instruction_str , synthesize_response=True)
population_pandas_query_engine.update_prompts({"pandas_prompt": new_prompt})

# Create tools for the agent
tools = [
        QueryEngineTool(
        query_engine=population_pandas_query_engine,
        metadata=ToolMetadata(
            name="population_data",
            description="Provides information on world population and demographics.",
        ),
        
    ),
    note_engine,

    # QueryEngineTool(
    #     query_engine=code_runner_engine,
    #     metadata=ToolMetadata(
    #         name="code_runner",
    #         description="Executes the code  which generated by llm in the terminal.",
    #     ),
    # ),
]
memory = ConversationBufferMemory()

# Initialize the agent
agent = ReActAgent(
    tools=tools,
    llm=llm,
    verbose=True,
    context='The agent assists users by providing accurate information about world population statistics, generating code, and executing it.',
    memory=None
)

# Main loop for user interaction
while (prompt := input("Enter a prompt (q to quit): ")) != "q":
    # Process the query using the agent

    result = agent.query(prompt)
    
    # Print the agent's response
    print("Agent Response:")
    print(result)



    # # Optionally save notes based on the user request
    # if "save note" in prompt.lower():
    #     note_content = input("Enter note content: ")
    #     note_engine.save(note_content)
    #     print("Note saved.")
